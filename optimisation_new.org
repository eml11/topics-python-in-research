Numerical Optimisation

* Representation of Functions
 * possibly should go here?

* Numerical Calculus
 * Needed for some of the optimisation algorythms
 * Different ways of calculating derivatives
 * Finite Difference, with different orders depending on number of neighbours
 * Difference Stencil
 * Collocation Methods, can represent function/data in terms of some basis 
  functions whose derivatives are easy to compute (in fact all numerical differentiation is this).
  
* General Problems as Optimisation Problems
 * Many problems can be rewritten as an optimisation problem
 * e.g.

 G x^{\prime} = d

 * Where G is an (in priciple nonlinear) opperator, and x^{\prime} and $d$ live in (different) vector spaces.
 * The dimensions of x^{\prime} and d need not be the same.
 * If x is a trial solution to the above then we have the residual

 r = G x - d

 * To solve have to find a way of improving our guess for x so that r=0

* The Objective Function
 * Normal way to do this is with an objective function

 \phi = |G x - d|_N
 
 * Where $|\cdot|_N$ is some appropriate norm
 * Now the solution corresponds to finding the minima of \phi i.e. find
 
  \frac{\delta \phi}{\delta x} = 0
  
 * Where the \delta denote functional derivatives
 * In minimising \phi we want to make use of the gradient information as this allows for faster convergence
 * Can get this numerically
 * Alternatively if \phi takes the form of an action then we can obtain this from a Euler-Lagrange Equation
 
* Regulerisation
 * In general G x = d is quite generic and can include massively over/underdetermined systems
 * $G$ can also be singular/near singular
 * This can lead to unstable solutions which can be highly non-smooth/contain very large values
 * The solution to this is regulerisation, we modify the objective function so that it penalises unwanted
   behaviour in the solution
 * e.g.
 
  \phi = |G x - d|_N + \mu_1 |x|_N + \mu_2 |\nabla x |_N
  
  * where \mu_1 and \mu_2 are trade off parameters
